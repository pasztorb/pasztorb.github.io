---
title: "Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning"
collection: publications
permalink: /publication/2023-06-29-safe-m3-ucrl

[//]: # (excerpt: 'This paper is about the number 1. The number 2 is left for future work.')
date: 2023-06-29
venue: 'ArXiv'
paperurl: 'https://arxiv.org/abs/2306.17052'
citation: 'Matej Jusup, Barna Pásztor, Tadeusz Janik, Kenan Zhang, Francesco Corman, Andreas Krause, Ilija Bogunovic (2023). &quot;Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning.&quot; <i>ArXiv</i>.'
---
[Download paper here](https://openreview.net/pdf?id=gvcDSDYUZx)

**Abstract**:
Many applications, e.g., in shared mobility, require coordinating a large number of agents. 
Mean-field reinforcement learning addresses the resulting scalability challenge by optimizing the policy of a 
representative agent. In this paper, we address an important generalization where there exist global constraints on the 
distribution of agents (e.g., requiring capacity constraints or minimum coverage requirements to be met). We propose 
Safe-M3-UCRL, the first model-based algorithm that attains safe policies even in the case of unknown transition dynamics. 
As a key ingredient, it uses epistemic uncertainty in the transition model within a log-barrier approach to ensure 
pessimistic constraints satisfaction with high probability. We showcase Safe-M3-UCRL on the vehicle repositioning 
problem faced by many shared mobility operators and evaluate its performance through simulations built on Shenzhen taxi 
trajectory data. Our algorithm effectively meets the demand in critical areas while ensuring service accessibility in 
regions with low demand.

Recommended citation: Matej Jusup, Barna Pásztor, Tadeusz Janik, Kenan Zhang, Francesco Corman, Andreas Krause, Ilija Bogunovic (2023). &quot;Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning.&quot; <i>ArXiv</i>.